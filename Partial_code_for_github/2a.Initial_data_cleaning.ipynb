{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59ec606",
   "metadata": {},
   "source": [
    "# Data Engineering - Initial Dataset Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7cc458",
   "metadata": {},
   "source": [
    "This notebook contains various functions I created to create a dataframe with cleaned comment data. It involved building functions, the function have documentation where neccesary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb37f33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\filip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\filip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\filip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import datetime\n",
    "import re\n",
    "from collections import Counter\n",
    "# from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#stuff from nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize #<- For finding tokens (small divisions) from a large sample of text\n",
    "from nltk.corpus import stopwords #<- For calling the know stopwords in english (e.g, articles, connectors)\n",
    "from nltk.corpus import wordnet #<- For calling a lexical database in eglish with meanings, synonyms, antonyms, and more \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "\n",
    "########## progress bar\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "import time\n",
    "\n",
    "sid_analyzer = SentimentIntensityAnalyzer()\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "##### emotions\n",
    "import text2emotion as te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b9691",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_cols = {\"author\": str, \"body\": str, \"created_utc\": str, \"score\": float}\n",
    "df = pd.read_csv('df.csv',  usecols=dt_cols, low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3939bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_removed_deleted(df):\n",
    "    \n",
    "    \"\"\"Input = uncleaned reddit data dataframe with a column called 'body'\n",
    "    \n",
    "    Output = body column with removed '[deleted]' '[removed]' entries which do not give us\n",
    "    anything of value for sentiment analysis\n",
    "    \n",
    "    '[deleted]' = deleted post by poster\n",
    "    '[removed]' = deleted by moderators, either auto or manually\n",
    "    \"\"\"\n",
    "    \n",
    "    # NA's\n",
    "    df = df.dropna(how='any')\n",
    "    \n",
    "    # Removing '[removed]' entries which create noise, account for about 8% of data\n",
    "    print('dropping [removed] posts')\n",
    "    removed = df[df.loc[:, 'body'].progress_apply(lambda x: str(x)==\"[removed]\")]\n",
    "    index_r = removed.index\n",
    "    cleaned_r = pd.DataFrame.drop(df, index = index_r)\n",
    "    \n",
    "    # Removing '[deleted]' entries to remove noise\n",
    "    print(' dropping [deleted] posts')\n",
    "    deleted = cleaned_r[cleaned_r.loc[:,'body'].progress_apply(lambda x: str(x)==\"[deleted]\")]\n",
    "    index_d = deleted.index\n",
    "    cleaned_d = pd.DataFrame.drop(cleaned_r, index = index_d)\n",
    "    cleaned = cleaned_d\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "cleaned_comments  = clean_removed_deleted(df.copy(deep=True))\n",
    "cleaned_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d7506",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_list = ['AutoModerator', '___alexa___', 'SwapzoneIO', 'ccModBot', 'coinfeeds-bot', 'CryptoMods','[deleted]', '[removed]']\n",
    "\n",
    "# users = df[\"author\"].value_counts()\n",
    "# users.head(50)\n",
    "\n",
    "def remove_bots(df, bot_list, column_name):\n",
    "    \n",
    "    \"\"\"This function is designed to compare authors of comments in dataframe with a predefined list of bot accounts\n",
    "    \n",
    "    inputs = dataframe, list of bots, column name (must be a string) in dataframe to compute comparison\n",
    "    \n",
    "    output = dataframe with comments by bot accounts removed\"\"\"\n",
    "    is_in = df[~df[column_name].isin(bot_list)]     \n",
    "    \n",
    "    return is_in\n",
    "\n",
    "removed_bots = remove_bots(df=cleaned_comments.copy(deep=True), bot_list = bot_list, column_name = 'author')\n",
    "removed_bots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d692770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hyperlinks(df, column_name):\n",
    "    \n",
    "    \"\"\"This function removed all hyperlinks that behin with 'http' and \n",
    "    replace with a whitespace and counts number of hyperlinks removed\n",
    "    \n",
    "    Input = Dataframe, selected column to remove hyperlinks\n",
    "    \n",
    "    Output = Dataframe with removed hyperlinks in text column\n",
    "    \"\"\"\n",
    "#     count = 0\n",
    "    \n",
    "#     for comment in df['body']:\n",
    "#         if (re.subn(r'http\\S+', \" \", comment)[1]) > 0:\n",
    "#             count+=1\n",
    "                 \n",
    "#     print(\"removed hyperlinks in {} rows/comments\".format(count))\n",
    "    \n",
    "    df[column_name] = df[column_name].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df1 = removed_bots.copy(deep=True)\n",
    "\n",
    "df2 = clean_hyperlinks(df1, column_name = 'body')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35004fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datetime(df):\n",
    "    \n",
    "    \"\"\"Reddit does not provide a datetime format, comments based on UTC format\n",
    "    Input = This function takes in Pandas dataframe and expects the presence of a 'created_utc' column\n",
    "    to convert to Datetime\n",
    "    \n",
    "    Output = New column in dataframe called 'datetime' with date present\"\"\"\n",
    "\n",
    "    df['created_utc'] = df['created_utc'].astype(int)\n",
    "    x = df['created_utc']\n",
    "    \n",
    "    datetime = []\n",
    "    print('creating datetime column')\n",
    "    for num in tqdm_notebook(x):\n",
    "        y = pd.Timestamp(num, unit='s', hour=None)\n",
    "        datetime.append(y)  \n",
    "    df['date'] = datetime\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "    df\n",
    "    \n",
    "    return df\n",
    "\n",
    "# df1 = removed_bots.copy(deep=True)\n",
    "\n",
    "datetime = get_datetime(df2)\n",
    "datetime#.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8821234",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df.groupby([pd.Grouper('date')]).count()#['date'].count()\n",
    "pd.set_option('display.max_rows', 4000)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b5ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text:str, analyser, desired_type:str='pos'):\n",
    "    # Get sentiment from text\n",
    "    sentiment_score = analyser.polarity_scores(text)\n",
    "    return sentiment_score[desired_type]\n",
    "\n",
    "def get_sentiment_scores(df,data_column):\n",
    "    print('Getting compound sentiment')\n",
    "    df['Compound Sentiment Score'] = df[data_column].astype(str).progress_apply(lambda x: get_sentiment(x,sid_analyzer,'compound'))\n",
    "    \n",
    "#     print('Getting compound sentiment')\n",
    "#     df['Positive Sentiment Score'] = df[data_column].astype(str).progress_apply(lambda x: get_sentiment(x,sid_analyzer,'pos'))\n",
    "    \n",
    "#     print('Getting compound sentiment')\n",
    "#     df['Negative Sentiment Score'] = df[data_column].astype(str).progress_apply(lambda x: get_sentiment(x,sid_analyzer,'neg'))\n",
    "    \n",
    "#     print('Getting compound sentiment')\n",
    "#     df['Neutral Sentiment Score'] = df[data_column].astype(str).progress_apply(lambda x: get_sentiment(x,sid_analyzer,'neu'))\n",
    "    return df\n",
    "\n",
    "# df_small\n",
    "\n",
    "df = get_sentiment_scores(datetime, 'body')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55635dbe",
   "metadata": {},
   "source": [
    "# Dataframe building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2f4edb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>date</th>\n",
       "      <th>Compound Sentiment Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TechnoMagik</td>\n",
       "      <td>I'm not sure how you eliminate spread.. If I a...</td>\n",
       "      <td>1368332818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-05-12</td>\n",
       "      <td>0.9014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mytwobitcents</td>\n",
       "      <td>fixed thanks</td>\n",
       "      <td>1368321753</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2013-05-12</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sex_with_a_goat</td>\n",
       "      <td>The Spanish one is wrong, we don't use 'y' wit...</td>\n",
       "      <td>1368318717</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2013-05-12</td>\n",
       "      <td>-0.4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>davidpbrown</td>\n",
       "      <td>Yes, Russian Trolls are the most obvious answer.</td>\n",
       "      <td>1368298185</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2013-05-11</td>\n",
       "      <td>0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bigglejones</td>\n",
       "      <td>A \"consultant\" asking advice on how to be a co...</td>\n",
       "      <td>1368257850</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2013-05-11</td>\n",
       "      <td>0.0813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4553702</th>\n",
       "      <td>Cold_Goose_4242</td>\n",
       "      <td>I want 75k end of week</td>\n",
       "      <td>1612828909</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2021-02-09</td>\n",
       "      <td>0.0772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4553703</th>\n",
       "      <td>damnusernamegotcutof</td>\n",
       "      <td>Have mercy oh crypto gods for I can only get s...</td>\n",
       "      <td>1612828904</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2021-02-09</td>\n",
       "      <td>0.3612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4553704</th>\n",
       "      <td>larrydavid4eyedfuck</td>\n",
       "      <td>yes thats correct, I own the ethereum wallet t...</td>\n",
       "      <td>1612828903</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-02-09</td>\n",
       "      <td>0.7608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4553705</th>\n",
       "      <td>ChocolateMorsels</td>\n",
       "      <td>This is insanity.  Thank you Elon, very cool!</td>\n",
       "      <td>1612828897</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2021-02-09</td>\n",
       "      <td>0.1742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4553706</th>\n",
       "      <td>Jooylo</td>\n",
       "      <td>$1,000,000 would make the market cap of BTC at...</td>\n",
       "      <td>1612828896</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2021-02-09</td>\n",
       "      <td>0.3384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4553707 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       author  \\\n",
       "0                 TechnoMagik   \n",
       "1               mytwobitcents   \n",
       "2             sex_with_a_goat   \n",
       "3                 davidpbrown   \n",
       "4                 bigglejones   \n",
       "...                       ...   \n",
       "4553702       Cold_Goose_4242   \n",
       "4553703  damnusernamegotcutof   \n",
       "4553704   larrydavid4eyedfuck   \n",
       "4553705      ChocolateMorsels   \n",
       "4553706                Jooylo   \n",
       "\n",
       "                                                      body  created_utc  \\\n",
       "0        I'm not sure how you eliminate spread.. If I a...   1368332818   \n",
       "1                                             fixed thanks   1368321753   \n",
       "2        The Spanish one is wrong, we don't use 'y' wit...   1368318717   \n",
       "3         Yes, Russian Trolls are the most obvious answer.   1368298185   \n",
       "4        A \"consultant\" asking advice on how to be a co...   1368257850   \n",
       "...                                                    ...          ...   \n",
       "4553702                             I want 75k end of week   1612828909   \n",
       "4553703  Have mercy oh crypto gods for I can only get s...   1612828904   \n",
       "4553704  yes thats correct, I own the ethereum wallet t...   1612828903   \n",
       "4553705      This is insanity.  Thank you Elon, very cool!   1612828897   \n",
       "4553706  $1,000,000 would make the market cap of BTC at...   1612828896   \n",
       "\n",
       "         score        date  Compound Sentiment Score  \n",
       "0          1.0  2013-05-12                    0.9014  \n",
       "1          2.0  2013-05-12                    0.4404  \n",
       "2          2.0  2013-05-12                   -0.4767  \n",
       "3          2.0  2013-05-11                    0.4019  \n",
       "4          2.0  2013-05-11                    0.0813  \n",
       "...        ...         ...                       ...  \n",
       "4553702    2.0  2021-02-09                    0.0772  \n",
       "4553703    2.0  2021-02-09                    0.3612  \n",
       "4553704    1.0  2021-02-09                    0.7608  \n",
       "4553705    3.0  2021-02-09                    0.1742  \n",
       "4553706    2.0  2021-02-09                    0.3384  \n",
       "\n",
       "[4553707 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment_df = df.copy(deep=True)\n",
    "sentiment_df = pd.read_csv('sentiment_df.csv')\n",
    "sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6312c5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>date</th>\n",
       "      <th>Compound Sentiment Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sex_with_a_goat</td>\n",
       "      <td>The Spanish one is wrong, we don't use 'y' wit...</td>\n",
       "      <td>1368318717</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2013-05-12</td>\n",
       "      <td>-0.4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>davidpbrown</td>\n",
       "      <td>Websites get hacked.. news at 11.\\n\\nDon't kee...</td>\n",
       "      <td>1368255591</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2013-05-11</td>\n",
       "      <td>-0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sgodsdogs</td>\n",
       "      <td>it seems there's been a lot of talking heads i...</td>\n",
       "      <td>1368249064</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2013-05-11</td>\n",
       "      <td>-0.5574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>hyh123</td>\n",
       "      <td>I see. But copycats of the world don't seem to...</td>\n",
       "      <td>1368117303</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-05-09</td>\n",
       "      <td>-0.8221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AltClubGirls</td>\n",
       "      <td>Sorry that setting was changed by mistake. It ...</td>\n",
       "      <td>1368080817</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-05-09</td>\n",
       "      <td>-0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4553700</th>\n",
       "      <td>Denaneha</td>\n",
       "      <td>Officially £1 Trillion market cap . CONGRATS C...</td>\n",
       "      <td>1612828918</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2021-02-09</td>\n",
       "      <td>0.8769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4553701</th>\n",
       "      <td>ReformedPony</td>\n",
       "      <td>what coin/coins would you guys put 500-1k into...</td>\n",
       "      <td>1612828916</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2021-02-09</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4553703</th>\n",
       "      <td>damnusernamegotcutof</td>\n",
       "      <td>Have mercy oh crypto gods for I can only get s...</td>\n",
       "      <td>1612828904</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2021-02-09</td>\n",
       "      <td>0.3612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4553704</th>\n",
       "      <td>larrydavid4eyedfuck</td>\n",
       "      <td>yes thats correct, I own the ethereum wallet t...</td>\n",
       "      <td>1612828903</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-02-09</td>\n",
       "      <td>0.7608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4553706</th>\n",
       "      <td>Jooylo</td>\n",
       "      <td>$1,000,000 would make the market cap of BTC at...</td>\n",
       "      <td>1612828896</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2021-02-09</td>\n",
       "      <td>0.3384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3765136 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       author  \\\n",
       "2             sex_with_a_goat   \n",
       "5                 davidpbrown   \n",
       "8                   sgodsdogs   \n",
       "23                     hyh123   \n",
       "26               AltClubGirls   \n",
       "...                       ...   \n",
       "4553700              Denaneha   \n",
       "4553701          ReformedPony   \n",
       "4553703  damnusernamegotcutof   \n",
       "4553704   larrydavid4eyedfuck   \n",
       "4553706                Jooylo   \n",
       "\n",
       "                                                      body  created_utc  \\\n",
       "2        The Spanish one is wrong, we don't use 'y' wit...   1368318717   \n",
       "5        Websites get hacked.. news at 11.\\n\\nDon't kee...   1368255591   \n",
       "8        it seems there's been a lot of talking heads i...   1368249064   \n",
       "23       I see. But copycats of the world don't seem to...   1368117303   \n",
       "26       Sorry that setting was changed by mistake. It ...   1368080817   \n",
       "...                                                    ...          ...   \n",
       "4553700  Officially £1 Trillion market cap . CONGRATS C...   1612828918   \n",
       "4553701  what coin/coins would you guys put 500-1k into...   1612828916   \n",
       "4553703  Have mercy oh crypto gods for I can only get s...   1612828904   \n",
       "4553704  yes thats correct, I own the ethereum wallet t...   1612828903   \n",
       "4553706  $1,000,000 would make the market cap of BTC at...   1612828896   \n",
       "\n",
       "         score        date  Compound Sentiment Score  \n",
       "2          2.0  2013-05-12                   -0.4767  \n",
       "5          3.0  2013-05-11                   -0.4019  \n",
       "8          5.0  2013-05-11                   -0.5574  \n",
       "23         1.0  2013-05-09                   -0.8221  \n",
       "26         1.0  2013-05-09                   -0.4019  \n",
       "...        ...         ...                       ...  \n",
       "4553700    5.0  2021-02-09                    0.8769  \n",
       "4553701    3.0  2021-02-09                    0.4404  \n",
       "4553703    2.0  2021-02-09                    0.3612  \n",
       "4553704    1.0  2021-02-09                    0.7608  \n",
       "4553706    2.0  2021-02-09                    0.3384  \n",
       "\n",
       "[3765136 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_positive = sentiment_df.copy(deep=True)\n",
    "df_negative = sentiment_df.copy(deep=True)\n",
    "\n",
    "df_positive_drop = df_positive[df_positive['Compound Sentiment Score'] <= 0.25].index\n",
    "df_positive_drop\n",
    "df_clean_pos = df_positive.drop(index = df_positive_drop)\n",
    "\n",
    "df_negative_drop = df_negative[df_negative['Compound Sentiment Score'] >= -0.25].index\n",
    "df_negative\n",
    "df_clean_neg = df_negative.drop(index = df_negative_drop)\n",
    "\n",
    "\n",
    "\n",
    "nonneutral = pd.concat((df_clean_neg, df_clean_pos))\n",
    "nonneutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31a61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neutral dropping\n",
    "# sentiment_df['Positive Compound'] = sentiment_df['Compound Sentiment Score'] >= 0.25\n",
    "# sentiment_df['Negative Compound'] = sentiment_df['Compound Sentiment Score'] <= -0.25\n",
    "# sentiment_df\n",
    "\n",
    "# neutral = sentiment_df[sentiment_df['Compound Sentiment Score'] == 0.0].index\n",
    "# neutral\n",
    "# df_clean = sentiment_df.drop(index = neutral)\n",
    "# df_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef6af98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonneutral.to_csv('sentiment_df_0_25.csv', header=True, index=False, columns=list(nonneutral.axes[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96dbedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = sentiment_df[sentiment_df['Compound Sentiment Score'] <= -0.25].index\n",
    "negative\n",
    "positive = sentiment_df[sentiment_df['Compound Sentiment Score'] >= 0.25].index\n",
    "positive\n",
    "\n",
    "# to_drop = negative.append(positive)\n",
    "# to_drop\n",
    "\n",
    "# df_clean = sentiment_df.drop(index = to_drop)\n",
    "# df_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4d2252",
   "metadata": {},
   "source": [
    "#  to do: drop all the wrongly binned comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25138f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentiment_df.csv', parse_dates=['date'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ac1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_median = df.groupby([pd.Grouper(key='date',freq='D')])['Compound Sentiment Score'].median()\n",
    "compound_median\n",
    "print(min(compound_median)),print(max(compound_median))\n",
    "\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9247dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in compound_median:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57399bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "median = pd.DataFrame(compound_median)\n",
    "for i in range(0,len(median)):\n",
    "    print([i])\n",
    "    print(median['Compound Sentiment Score'][i])\n",
    "    print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5955134",
   "metadata": {},
   "outputs": [],
   "source": [
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated = compound_median.interpolate(method = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb59e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_change = []\n",
    "\n",
    "for i in range(0,len(interpolated)-1):\n",
    "    change = (interpolated[i+1]-interpolated[i])\n",
    "    sent_change.append(change)\n",
    "#     print(interpolated[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db50647",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a5d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_change.insert(0,0)\n",
    "len(sent_change)\n",
    "median['sentiment change'] = sent_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea3f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "median['Compound Sentiment Score'] = interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7589d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = median.index\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4046fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "median['date'] = date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd1749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "median.to_csv('sentiment_daily.csv', header=True, index=False, columns=list(median.axes[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c1e3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
